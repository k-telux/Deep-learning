{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dce272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Training on 800 examples\n",
      "Using both high and low level features\n",
      "Testing on 200 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.732454\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.640613\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.723948\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.600943\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.749902\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.787560\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.711694\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.745844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6872, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.654344\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.548778\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.647273\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.774238\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.664104\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.755661\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.625494\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.672991\n",
      "\n",
      "Test set: Average loss: 0.6868, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.697225\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.677755\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.691703\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.816588\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.652826\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.697007\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.677167\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.721561\n",
      "\n",
      "Test set: Average loss: 0.6865, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.652867\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.717385\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.781691\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.677011\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.651080\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.686692\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.716992\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.718167\n",
      "\n",
      "Test set: Average loss: 0.6862, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.708454\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.680262\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.655941\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.691977\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.714685\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.700205\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.816368\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.697370\n",
      "\n",
      "Test set: Average loss: 0.6859, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.705387\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.750989\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.665423\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.615545\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.667823\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.678533\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.652700\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.716482\n",
      "\n",
      "Test set: Average loss: 0.6855, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.701563\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.792097\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.694652\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.744504\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.604084\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.642268\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.638980\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.810414\n",
      "\n",
      "Test set: Average loss: 0.6852, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.672785\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.663524\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.634390\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.782436\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.622931\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.798363\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.849115\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.845939\n",
      "\n",
      "Test set: Average loss: 0.6849, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.602109\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.645603\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.741594\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.838609\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.766775\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.714559\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.647362\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.772448\n",
      "\n",
      "Test set: Average loss: 0.6846, Accuracy: 109/200 (54.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.746650\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.666857\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.719591\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.722658\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.697157\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.649453\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.722224\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.683559\n",
      "\n",
      "Test set: Average loss: 0.6843, Accuracy: 110/200 (55.000%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.660098\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.779229\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.800567\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.792650\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.704033\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.738051\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.776444\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.715671\n",
      "\n",
      "Test set: Average loss: 0.7005, Accuracy: 90/200 (45.000%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.659105\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.769081\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.651384\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.751032\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.682991\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.729685\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.668702\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.813128\n",
      "\n",
      "Test set: Average loss: 0.6986, Accuracy: 90/200 (45.000%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.676234\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.684061\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.719320\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.709497\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.791920\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.706743\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.678540\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.675595\n",
      "\n",
      "Test set: Average loss: 0.6969, Accuracy: 90/200 (45.000%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.718256\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.709509\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.785680\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.648569\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.739486\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.660033\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.652179\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.715216\n",
      "\n",
      "Test set: Average loss: 0.6954, Accuracy: 91/200 (45.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.730351\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.684815\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.692975\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.685488\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.690753\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.632848\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.772072\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.630397\n",
      "\n",
      "Test set: Average loss: 0.6941, Accuracy: 92/200 (46.000%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.761010\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.655912\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.734822\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.712295\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.681834\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.762394\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.723759\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.710477\n",
      "\n",
      "Test set: Average loss: 0.6927, Accuracy: 96/200 (48.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.643190\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.688803\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.622115\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.709984\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.691399\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.703251\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.687052\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.682377\n",
      "\n",
      "Test set: Average loss: 0.6914, Accuracy: 97/200 (48.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.667210\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.720413\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.761852\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.696172\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.676219\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.675291\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.717910\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.741575\n",
      "\n",
      "Test set: Average loss: 0.6900, Accuracy: 104/200 (52.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.720216\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.699714\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.663422\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.710560\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.685932\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.677406\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.697633\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.708912\n",
      "\n",
      "Test set: Average loss: 0.6889, Accuracy: 107/200 (53.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.672557\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.712942\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.709289\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.763107\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.759271\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.660379\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.771813\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.741453\n",
      "\n",
      "Test set: Average loss: 0.6877, Accuracy: 111/200 (55.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.535940\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.702250\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.708846\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.720339\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.700032\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.677381\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.683872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.706530\n",
      "\n",
      "Test set: Average loss: 0.6795, Accuracy: 112/200 (56.000%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.692189\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.677085\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.681685\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.726102\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.630785\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.705187\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.717013\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.661071\n",
      "\n",
      "Test set: Average loss: 0.6699, Accuracy: 131/200 (65.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.685361\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.633261\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.625551\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.680446\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.739959\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.697514\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.570161\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.637917\n",
      "\n",
      "Test set: Average loss: 0.6617, Accuracy: 135/200 (67.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.639979\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.680582\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.681639\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.689007\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.646747\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.622378\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.655307\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.703663\n",
      "\n",
      "Test set: Average loss: 0.6541, Accuracy: 137/200 (68.500%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.651343\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.689169\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.638947\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.636016\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.651826\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.650439\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.614703\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.699093\n",
      "\n",
      "Test set: Average loss: 0.6461, Accuracy: 139/200 (69.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.649968\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.641780\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.702671\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.691049\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.685912\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.738020\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.616360\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.647227\n",
      "\n",
      "Test set: Average loss: 0.6391, Accuracy: 142/200 (71.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.674478\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.664851\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.726786\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.620805\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.575266\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.626967\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.680565\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.720018\n",
      "\n",
      "Test set: Average loss: 0.6311, Accuracy: 144/200 (72.000%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.706053\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.586083\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.593904\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.651202\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.709009\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.637354\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.611345\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.585281\n",
      "\n",
      "Test set: Average loss: 0.6227, Accuracy: 145/200 (72.500%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.701501\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.607780\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.690965\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.664506\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.602684\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.712944\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.649828\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.729722\n",
      "\n",
      "Test set: Average loss: 0.6165, Accuracy: 144/200 (72.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.611948\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.630205\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.738569\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.734547\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.598951\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.696286\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.622088\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.598491\n",
      "\n",
      "Test set: Average loss: 0.6087, Accuracy: 147/200 (73.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.740682\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.709194\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.700593\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.696298\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.645403\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.672760\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.717623\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.667482\n",
      "\n",
      "Test set: Average loss: 0.6404, Accuracy: 133/200 (66.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.741061\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.597693\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 0.756894\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.576508\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.684803\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.620669\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.576923\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.624852\n",
      "\n",
      "Test set: Average loss: 0.5885, Accuracy: 147/200 (73.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.638178\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.635539\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.435137\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.706154\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.608049\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.568599\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.703634\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.440436\n",
      "\n",
      "Test set: Average loss: 0.5505, Accuracy: 147/200 (73.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.455006\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.556313\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.602725\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.767345\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.552315\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 0.364671\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.463387\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.693603\n",
      "\n",
      "Test set: Average loss: 0.5182, Accuracy: 150/200 (75.000%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.428877\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.604676\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.619490\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.835538\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.648645\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.500408\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.822991\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.320443\n",
      "\n",
      "Test set: Average loss: 0.5139, Accuracy: 156/200 (78.000%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.615003\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.427387\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.529825\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.223804\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.535828\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.666065\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.602768\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.368306\n",
      "\n",
      "Test set: Average loss: 0.5321, Accuracy: 145/200 (72.500%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.457206\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.627245\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.474246\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.637998\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.432712\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.447536\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.776357\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.342265\n",
      "\n",
      "Test set: Average loss: 0.5052, Accuracy: 148/200 (74.000%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.515144\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.352444\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.458266\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.605224\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.557277\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.444514\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.310954\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.823408\n",
      "\n",
      "Test set: Average loss: 0.5150, Accuracy: 154/200 (77.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.616558\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.488706\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.389144\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.533715\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.474486\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.479782\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.360344\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.720630\n",
      "\n",
      "Test set: Average loss: 0.5025, Accuracy: 154/200 (77.000%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.405143\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.180535\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.753984\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.315640\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.271023\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.381826\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.396175\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.851097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4935, Accuracy: 153/200 (76.500%)\n",
      "\n",
      "\n",
      " training DNN with  1000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.716059\n",
      "Train Epoch: 1 [100/800 (12%)]\tLoss: 0.692431\n",
      "Train Epoch: 1 [200/800 (25%)]\tLoss: 0.585298\n",
      "Train Epoch: 1 [300/800 (38%)]\tLoss: 0.452886\n",
      "Train Epoch: 1 [400/800 (50%)]\tLoss: 0.741964\n",
      "Train Epoch: 1 [500/800 (62%)]\tLoss: 0.541759\n",
      "Train Epoch: 1 [600/800 (75%)]\tLoss: 0.572539\n",
      "Train Epoch: 1 [700/800 (88%)]\tLoss: 0.703209\n",
      "\n",
      "Test set: Average loss: 0.5656, Accuracy: 139/200 (69.500%)\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.539241\n",
      "Train Epoch: 2 [100/800 (12%)]\tLoss: 0.727003\n",
      "Train Epoch: 2 [200/800 (25%)]\tLoss: 1.787122\n",
      "Train Epoch: 2 [300/800 (38%)]\tLoss: 0.771827\n",
      "Train Epoch: 2 [400/800 (50%)]\tLoss: 0.430325\n",
      "Train Epoch: 2 [500/800 (62%)]\tLoss: 0.482922\n",
      "Train Epoch: 2 [600/800 (75%)]\tLoss: 0.905735\n",
      "Train Epoch: 2 [700/800 (88%)]\tLoss: 0.860607\n",
      "\n",
      "Test set: Average loss: 0.6474, Accuracy: 125/200 (62.500%)\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.515047\n",
      "Train Epoch: 3 [100/800 (12%)]\tLoss: 0.973538\n",
      "Train Epoch: 3 [200/800 (25%)]\tLoss: 0.627110\n",
      "Train Epoch: 3 [300/800 (38%)]\tLoss: 0.483454\n",
      "Train Epoch: 3 [400/800 (50%)]\tLoss: 0.712891\n",
      "Train Epoch: 3 [500/800 (62%)]\tLoss: 0.509127\n",
      "Train Epoch: 3 [600/800 (75%)]\tLoss: 0.357185\n",
      "Train Epoch: 3 [700/800 (88%)]\tLoss: 0.371594\n",
      "\n",
      "Test set: Average loss: 0.6072, Accuracy: 129/200 (64.500%)\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.347159\n",
      "Train Epoch: 4 [100/800 (12%)]\tLoss: 0.517832\n",
      "Train Epoch: 4 [200/800 (25%)]\tLoss: 0.398167\n",
      "Train Epoch: 4 [300/800 (38%)]\tLoss: 0.776988\n",
      "Train Epoch: 4 [400/800 (50%)]\tLoss: 0.673486\n",
      "Train Epoch: 4 [500/800 (62%)]\tLoss: 1.007521\n",
      "Train Epoch: 4 [600/800 (75%)]\tLoss: 0.751976\n",
      "Train Epoch: 4 [700/800 (88%)]\tLoss: 0.493931\n",
      "\n",
      "Test set: Average loss: 0.5462, Accuracy: 148/200 (74.000%)\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.698860\n",
      "Train Epoch: 5 [100/800 (12%)]\tLoss: 0.581104\n",
      "Train Epoch: 5 [200/800 (25%)]\tLoss: 0.374833\n",
      "Train Epoch: 5 [300/800 (38%)]\tLoss: 0.469597\n",
      "Train Epoch: 5 [400/800 (50%)]\tLoss: 0.312239\n",
      "Train Epoch: 5 [500/800 (62%)]\tLoss: 0.405537\n",
      "Train Epoch: 5 [600/800 (75%)]\tLoss: 0.342701\n",
      "Train Epoch: 5 [700/800 (88%)]\tLoss: 0.901588\n",
      "\n",
      "Test set: Average loss: 0.6298, Accuracy: 123/200 (61.500%)\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.504805\n",
      "Train Epoch: 6 [100/800 (12%)]\tLoss: 0.404258\n",
      "Train Epoch: 6 [200/800 (25%)]\tLoss: 0.517038\n",
      "Train Epoch: 6 [300/800 (38%)]\tLoss: 0.850216\n",
      "Train Epoch: 6 [400/800 (50%)]\tLoss: 0.544712\n",
      "Train Epoch: 6 [500/800 (62%)]\tLoss: 0.601410\n",
      "Train Epoch: 6 [600/800 (75%)]\tLoss: 0.648892\n",
      "Train Epoch: 6 [700/800 (88%)]\tLoss: 0.698358\n",
      "\n",
      "Test set: Average loss: 0.7055, Accuracy: 126/200 (63.000%)\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.596653\n",
      "Train Epoch: 7 [100/800 (12%)]\tLoss: 0.488990\n",
      "Train Epoch: 7 [200/800 (25%)]\tLoss: 0.382831\n",
      "Train Epoch: 7 [300/800 (38%)]\tLoss: 0.484787\n",
      "Train Epoch: 7 [400/800 (50%)]\tLoss: 0.696362\n",
      "Train Epoch: 7 [500/800 (62%)]\tLoss: 0.849227\n",
      "Train Epoch: 7 [600/800 (75%)]\tLoss: 0.477391\n",
      "Train Epoch: 7 [700/800 (88%)]\tLoss: 0.655428\n",
      "\n",
      "Test set: Average loss: 0.5831, Accuracy: 131/200 (65.500%)\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.523609\n",
      "Train Epoch: 8 [100/800 (12%)]\tLoss: 0.475870\n",
      "Train Epoch: 8 [200/800 (25%)]\tLoss: 0.435474\n",
      "Train Epoch: 8 [300/800 (38%)]\tLoss: 0.529913\n",
      "Train Epoch: 8 [400/800 (50%)]\tLoss: 0.385561\n",
      "Train Epoch: 8 [500/800 (62%)]\tLoss: 0.682371\n",
      "Train Epoch: 8 [600/800 (75%)]\tLoss: 0.236000\n",
      "Train Epoch: 8 [700/800 (88%)]\tLoss: 0.520412\n",
      "\n",
      "Test set: Average loss: 0.6023, Accuracy: 130/200 (65.000%)\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.794128\n",
      "Train Epoch: 9 [100/800 (12%)]\tLoss: 0.468181\n",
      "Train Epoch: 9 [200/800 (25%)]\tLoss: 0.309259\n",
      "Train Epoch: 9 [300/800 (38%)]\tLoss: 0.645687\n",
      "Train Epoch: 9 [400/800 (50%)]\tLoss: 0.479925\n",
      "Train Epoch: 9 [500/800 (62%)]\tLoss: 0.342168\n",
      "Train Epoch: 9 [600/800 (75%)]\tLoss: 0.470608\n",
      "Train Epoch: 9 [700/800 (88%)]\tLoss: 0.494849\n",
      "\n",
      "Test set: Average loss: 0.5772, Accuracy: 139/200 (69.500%)\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.451669\n",
      "Train Epoch: 10 [100/800 (12%)]\tLoss: 0.461000\n",
      "Train Epoch: 10 [200/800 (25%)]\tLoss: 0.329423\n",
      "Train Epoch: 10 [300/800 (38%)]\tLoss: 0.319425\n",
      "Train Epoch: 10 [400/800 (50%)]\tLoss: 0.716864\n",
      "Train Epoch: 10 [500/800 (62%)]\tLoss: 0.348119\n",
      "Train Epoch: 10 [600/800 (75%)]\tLoss: 0.407119\n",
      "Train Epoch: 10 [700/800 (88%)]\tLoss: 0.445662\n",
      "\n",
      "Test set: Average loss: 0.5937, Accuracy: 141/200 (70.500%)\n",
      "\n",
      "Training on 8000 examples\n",
      "Using both high and low level features\n",
      "Testing on 2000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.707689\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.686874\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.687378\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.694385\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.687705\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.686235\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.723062\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.682868\n",
      "\n",
      "Test set: Average loss: 0.6885, Accuracy: 1070/2000 (53.500%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.696528\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.716871\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.697210\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.693378\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.696747\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.663673\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.687764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.702713\n",
      "\n",
      "Test set: Average loss: 0.6884, Accuracy: 1070/2000 (53.500%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.692937\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.702793\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.697296\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.701563\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.678569\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.691374\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.708445\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.700494\n",
      "\n",
      "Test set: Average loss: 0.6883, Accuracy: 1071/2000 (53.550%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.671973\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.699267\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.679213\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.678393\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.686321\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.708920\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.699830\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.680050\n",
      "\n",
      "Test set: Average loss: 0.6882, Accuracy: 1072/2000 (53.600%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.679362\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.710359\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.697500\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.686373\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.686935\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.689706\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.709695\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.696820\n",
      "\n",
      "Test set: Average loss: 0.6881, Accuracy: 1072/2000 (53.600%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.706133\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.691735\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.702001\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.702796\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.691176\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.674268\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.709720\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.692871\n",
      "\n",
      "Test set: Average loss: 0.6880, Accuracy: 1074/2000 (53.700%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.693140\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.695928\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.714230\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.710936\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.698636\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.700908\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.734433\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.687952\n",
      "\n",
      "Test set: Average loss: 0.6878, Accuracy: 1074/2000 (53.700%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.694346\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.685177\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.694600\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.692568\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.688053\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.700829\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.677457\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.722393\n",
      "\n",
      "Test set: Average loss: 0.6877, Accuracy: 1078/2000 (53.900%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.698391\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.708934\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.711654\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.686718\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.697003\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.704086\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.688142\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.728217\n",
      "\n",
      "Test set: Average loss: 0.6876, Accuracy: 1079/2000 (53.950%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.716802\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.713494\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.693415\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.703333\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.691261\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.712231\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.684283\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.677745\n",
      "\n",
      "Test set: Average loss: 0.6875, Accuracy: 1078/2000 (53.900%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.713819\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.700339\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.742133\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.712135\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.703536\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.703607\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.728841\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.738700\n",
      "\n",
      "Test set: Average loss: 0.7072, Accuracy: 773/2000 (38.650%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.717398\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.729905\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.704101\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.734782\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.709969\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.729604\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.702071\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.685780\n",
      "\n",
      "Test set: Average loss: 0.7040, Accuracy: 809/2000 (40.450%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.734880\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.711490\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.718750\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.714521\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.702005\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.710926\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.740980\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.721209\n",
      "\n",
      "Test set: Average loss: 0.7013, Accuracy: 875/2000 (43.750%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.720843\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.744687\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.707141\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.706155\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.688046\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.722207\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.686661\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.709210\n",
      "\n",
      "Test set: Average loss: 0.6990, Accuracy: 890/2000 (44.500%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.732290\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.727790\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.685829\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.721022\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.699535\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.729437\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.716541\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.735955\n",
      "\n",
      "Test set: Average loss: 0.6969, Accuracy: 909/2000 (45.450%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.694754\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.712016\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.720573\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.701492\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.704522\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.723612\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.728140\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.711686\n",
      "\n",
      "Test set: Average loss: 0.6950, Accuracy: 920/2000 (46.000%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.724774\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.695368\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.720010\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.686405\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.718417\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.693419\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.719702\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.685569\n",
      "\n",
      "Test set: Average loss: 0.6932, Accuracy: 936/2000 (46.800%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.674615\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.714308\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.694635\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.702620\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.717189\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.725562\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.712477\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.710868\n",
      "\n",
      "Test set: Average loss: 0.6916, Accuracy: 939/2000 (46.950%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.694986\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.708001\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.708070\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.684844\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.692933\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.700638\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.688772\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.697725\n",
      "\n",
      "Test set: Average loss: 0.6900, Accuracy: 940/2000 (47.000%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.700752\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.726043\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.704304\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.717704\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.670264\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.695572\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.688981\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.718423\n",
      "\n",
      "Test set: Average loss: 0.6885, Accuracy: 944/2000 (47.200%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.703644\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.694943\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.698409\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.713499\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.704816\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.706966\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.691603\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.701215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6935, Accuracy: 937/2000 (46.850%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.699657\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.697783\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.688749\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.694131\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.707187\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.695551\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.691903\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.695444\n",
      "\n",
      "Test set: Average loss: 0.6838, Accuracy: 1086/2000 (54.300%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.669780\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.689023\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.691339\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.657360\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.689147\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.679635\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.686098\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.696656\n",
      "\n",
      "Test set: Average loss: 0.6755, Accuracy: 1218/2000 (60.900%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.694198\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.687975\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.653688\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.686138\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.680615\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.665059\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.680905\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.670836\n",
      "\n",
      "Test set: Average loss: 0.6679, Accuracy: 1334/2000 (66.700%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.660664\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.670040\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.679925\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.686827\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.669173\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.666946\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.656945\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.667808\n",
      "\n",
      "Test set: Average loss: 0.6603, Accuracy: 1377/2000 (68.850%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.673013\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.661532\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.658468\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.647271\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.674121\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.655968\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.680404\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.688957\n",
      "\n",
      "Test set: Average loss: 0.6527, Accuracy: 1444/2000 (72.200%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.658154\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.675937\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.662239\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.665798\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.676299\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.658026\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.617592\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.651227\n",
      "\n",
      "Test set: Average loss: 0.6442, Accuracy: 1453/2000 (72.650%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.646950\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.635106\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.643387\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.642455\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.637406\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.647995\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.624906\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.672991\n",
      "\n",
      "Test set: Average loss: 0.6362, Accuracy: 1479/2000 (73.950%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.631008\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.643222\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.638166\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.636200\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.631096\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.672582\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.631630\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.633719\n",
      "\n",
      "Test set: Average loss: 0.6289, Accuracy: 1502/2000 (75.100%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.639842\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.639717\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.644197\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.652618\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.653150\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.644020\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.637691\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.639863\n",
      "\n",
      "Test set: Average loss: 0.6206, Accuracy: 1508/2000 (75.400%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.684952\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.676767\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.696879\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.691005\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.648616\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.639428\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.662720\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.622878\n",
      "\n",
      "Test set: Average loss: 0.6092, Accuracy: 1434/2000 (71.700%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.639237\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.607836\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.605926\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.593270\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.639207\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.601536\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.553499\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.602627\n",
      "\n",
      "Test set: Average loss: 0.5450, Accuracy: 1485/2000 (74.250%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.571219\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.668175\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.575095\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.589118\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.547299\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.590653\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.528154\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.595002\n",
      "\n",
      "Test set: Average loss: 0.5060, Accuracy: 1562/2000 (78.100%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.550709\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.572576\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.564062\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.565668\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.505528\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.540917\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.526960\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.605098\n",
      "\n",
      "Test set: Average loss: 0.4866, Accuracy: 1551/2000 (77.550%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.580497\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.516538\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.546932\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.593779\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.600204\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.525640\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.560615\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.559901\n",
      "\n",
      "Test set: Average loss: 0.4799, Accuracy: 1557/2000 (77.850%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.558557\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.499294\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.456013\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.449980\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.535585\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.516948\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.484795\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.566606\n",
      "\n",
      "Test set: Average loss: 0.4711, Accuracy: 1566/2000 (78.300%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.514925\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.477355\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.559769\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.467443\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.520916\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.565035\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.546554\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.558526\n",
      "\n",
      "Test set: Average loss: 0.4669, Accuracy: 1565/2000 (78.250%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.478705\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.479050\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.588007\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.551804\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.530851\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.532860\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.541079\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.514340\n",
      "\n",
      "Test set: Average loss: 0.4687, Accuracy: 1554/2000 (77.700%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.567549\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.569697\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.461772\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.431759\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.471051\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.534376\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.494993\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.415378\n",
      "\n",
      "Test set: Average loss: 0.4606, Accuracy: 1581/2000 (79.050%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.483377\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.502104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.471068\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.429322\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.509869\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.519110\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.421653\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.459364\n",
      "\n",
      "Test set: Average loss: 0.4597, Accuracy: 1582/2000 (79.100%)\n",
      "\n",
      "\n",
      " training DNN with 10000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.692278\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.634933\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.548177\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.523132\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.526403\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.519875\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.636333\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.541686\n",
      "\n",
      "Test set: Average loss: 0.4840, Accuracy: 1529/2000 (76.450%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.582702\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.561463\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.434947\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.471895\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.487997\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.462024\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.408990\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.537944\n",
      "\n",
      "Test set: Average loss: 0.4614, Accuracy: 1565/2000 (78.250%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.488829\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.367280\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.555966\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.475964\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.496502\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.489480\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.507322\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.417897\n",
      "\n",
      "Test set: Average loss: 0.4572, Accuracy: 1571/2000 (78.550%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.500067\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.475453\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.468698\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.406998\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.385067\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.463204\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.475962\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.458508\n",
      "\n",
      "Test set: Average loss: 0.4635, Accuracy: 1572/2000 (78.600%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.608916\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.442314\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.429127\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.467494\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.327072\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.406393\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.436388\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.545133\n",
      "\n",
      "Test set: Average loss: 0.4514, Accuracy: 1599/2000 (79.950%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.512305\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.633305\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.397317\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.472514\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.533562\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.434650\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.487640\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.457070\n",
      "\n",
      "Test set: Average loss: 0.4555, Accuracy: 1560/2000 (78.000%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.424008\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.456465\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.423169\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.440811\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.481629\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.553266\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.462194\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.480593\n",
      "\n",
      "Test set: Average loss: 0.4556, Accuracy: 1572/2000 (78.600%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.571256\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.508129\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.418616\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.448997\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.432726\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.451361\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.483763\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.468549\n",
      "\n",
      "Test set: Average loss: 0.4500, Accuracy: 1587/2000 (79.350%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.435954\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.560030\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.520223\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.477063\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.501383\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.438951\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.379745\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.460475\n",
      "\n",
      "Test set: Average loss: 0.4465, Accuracy: 1580/2000 (79.000%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.508786\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.436369\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.402724\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.413937\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.390606\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.592875\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.445514\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.365209\n",
      "\n",
      "Test set: Average loss: 0.4473, Accuracy: 1585/2000 (79.250%)\n",
      "\n",
      "Training on 80000 examples\n",
      "Using both high and low level features\n",
      "Testing on 20000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.712320\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.709235\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.704198\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.707029\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.705508\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.709099\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.720697\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.710354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6990, Accuracy: 9115/20000 (45.575%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.712005\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.709515\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.703378\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.706835\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.711065\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.704973\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.708498\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.702559\n",
      "\n",
      "Test set: Average loss: 0.6989, Accuracy: 9124/20000 (45.620%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.709688\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.698369\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.708620\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.701137\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.706827\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.709733\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.712157\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.708810\n",
      "\n",
      "Test set: Average loss: 0.6987, Accuracy: 9129/20000 (45.645%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.701320\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.710113\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.716104\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.698509\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.704281\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.708254\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.707493\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.705282\n",
      "\n",
      "Test set: Average loss: 0.6986, Accuracy: 9136/20000 (45.680%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.706883\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.704081\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.705153\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.705827\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.700793\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.706583\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.706671\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.702352\n",
      "\n",
      "Test set: Average loss: 0.6985, Accuracy: 9133/20000 (45.665%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.707261\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.700189\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.706583\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.703877\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.716173\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.705833\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.709413\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.707138\n",
      "\n",
      "Test set: Average loss: 0.6983, Accuracy: 9138/20000 (45.690%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.706059\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.703523\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.706987\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.703350\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.705473\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.704993\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.699045\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.706441\n",
      "\n",
      "Test set: Average loss: 0.6982, Accuracy: 9140/20000 (45.700%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.704002\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.711568\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.708251\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.705817\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.702479\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.707520\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.706731\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.701257\n",
      "\n",
      "Test set: Average loss: 0.6981, Accuracy: 9140/20000 (45.700%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.710687\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.707318\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.704763\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.701001\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.699850\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.706525\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.700408\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.713226\n",
      "\n",
      "Test set: Average loss: 0.6980, Accuracy: 9148/20000 (45.740%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.703594\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.709644\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.693253\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.716358\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.706559\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.708298\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.713821\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.712546\n",
      "\n",
      "Test set: Average loss: 0.6978, Accuracy: 9153/20000 (45.765%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.712915\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.710241\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.704934\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.722390\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.718568\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.714583\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.710678\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.710025\n",
      "\n",
      "Test set: Average loss: 0.7084, Accuracy: 9104/20000 (45.520%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.711268\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.722132\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.716495\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.718949\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.710160\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.713232\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.719120\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.713882\n",
      "\n",
      "Test set: Average loss: 0.7054, Accuracy: 9104/20000 (45.520%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.708820\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.710321\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.717327\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.711167\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.710396\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.713133\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.708359\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.711426\n",
      "\n",
      "Test set: Average loss: 0.7028, Accuracy: 9103/20000 (45.515%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.709083\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.709356\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.707676\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.706790\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.710470\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.702775\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.701699\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.706828\n",
      "\n",
      "Test set: Average loss: 0.7005, Accuracy: 9102/20000 (45.510%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.702822\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.704161\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.700623\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.706125\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.705501\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.707366\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.709216\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.712785\n",
      "\n",
      "Test set: Average loss: 0.6985, Accuracy: 9105/20000 (45.525%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.701019\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.702678\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.703550\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.702236\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.702370\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.704283\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.702423\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.706899\n",
      "\n",
      "Test set: Average loss: 0.6967, Accuracy: 9109/20000 (45.545%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.712391\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.702606\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.703967\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.703346\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.699838\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.699635\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.707810\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.700036\n",
      "\n",
      "Test set: Average loss: 0.6951, Accuracy: 9117/20000 (45.585%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.703085\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.700083\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.703250\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.692861\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.699022\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.699371\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.709740\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.697041\n",
      "\n",
      "Test set: Average loss: 0.6936, Accuracy: 9145/20000 (45.725%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.703388\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.699411\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.691944\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.703217\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.701585\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.705260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.709351\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.692558\n",
      "\n",
      "Test set: Average loss: 0.6921, Accuracy: 9194/20000 (45.970%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.698332\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.701813\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.699451\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.696051\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.703605\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.697048\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.696049\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.693648\n",
      "\n",
      "Test set: Average loss: 0.6908, Accuracy: 9276/20000 (46.380%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.712505\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.709667\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.696846\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.707485\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.699844\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.699555\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.698996\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.703894\n",
      "\n",
      "Test set: Average loss: 0.6879, Accuracy: 10425/20000 (52.125%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.690749\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.693704\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.688870\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.698410\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.697807\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.692571\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.683771\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.683695\n",
      "\n",
      "Test set: Average loss: 0.6747, Accuracy: 12068/20000 (60.340%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.685666\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.681182\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.683028\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.688219\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.677538\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.680703\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.677780\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.677478\n",
      "\n",
      "Test set: Average loss: 0.6632, Accuracy: 13268/20000 (66.340%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.673810\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.672177\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.669909\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.663748\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.675337\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.679560\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.671704\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.666497\n",
      "\n",
      "Test set: Average loss: 0.6526, Accuracy: 13966/20000 (69.830%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.659952\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.661877\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.659664\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.669239\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.659227\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.664482\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.653186\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.662164\n",
      "\n",
      "Test set: Average loss: 0.6424, Accuracy: 14321/20000 (71.605%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.655620\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.654529\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.649399\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.652624\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.650101\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.650192\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.647740\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.643340\n",
      "\n",
      "Test set: Average loss: 0.6327, Accuracy: 14548/20000 (72.740%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.652201\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.651889\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.648696\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.650281\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.642543\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.649568\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.637979\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.637202\n",
      "\n",
      "Test set: Average loss: 0.6232, Accuracy: 14701/20000 (73.505%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.640398\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.641361\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.639076\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.630137\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.639494\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.639365\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.643358\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.635929\n",
      "\n",
      "Test set: Average loss: 0.6144, Accuracy: 14795/20000 (73.975%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.643575\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.633227\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.638068\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.642270\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.639381\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.631965\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.638641\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.630283\n",
      "\n",
      "Test set: Average loss: 0.6057, Accuracy: 14870/20000 (74.350%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.620889\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.629697\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.623760\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.624397\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.627070\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.618799\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.622614\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.631885\n",
      "\n",
      "Test set: Average loss: 0.5974, Accuracy: 14925/20000 (74.625%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.704545\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.686879\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.670450\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.657353\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.654273\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.636372\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.629200\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.614672\n",
      "\n",
      "Test set: Average loss: 0.5857, Accuracy: 15090/20000 (75.450%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.610534\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.602068\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.602018\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.600978\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.585033\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.581228\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.576015\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.588609\n",
      "\n",
      "Test set: Average loss: 0.5286, Accuracy: 15281/20000 (76.405%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.564505\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.568325\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.566901\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.565763\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.566835\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.570369\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.570364\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.554942\n",
      "\n",
      "Test set: Average loss: 0.5035, Accuracy: 15407/20000 (77.035%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.551208\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.537837\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.548154\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.530149\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.534341\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.550588\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.534795\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.548783\n",
      "\n",
      "Test set: Average loss: 0.4895, Accuracy: 15493/20000 (77.465%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.537186\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.509394\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.550624\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.546677\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.503267\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.525060\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.507931\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.510288\n",
      "\n",
      "Test set: Average loss: 0.4819, Accuracy: 15537/20000 (77.685%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.518623\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.535491\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.513945\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.514369\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.489814\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.503044\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.508051\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.522801\n",
      "\n",
      "Test set: Average loss: 0.4760, Accuracy: 15604/20000 (78.020%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.512889\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.520764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.515223\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.539617\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.541621\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.494300\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.527946\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.497120\n",
      "\n",
      "Test set: Average loss: 0.4722, Accuracy: 15624/20000 (78.120%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.518530\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.521079\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.510530\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.512459\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.530596\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.493323\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.516939\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.495011\n",
      "\n",
      "Test set: Average loss: 0.4689, Accuracy: 15640/20000 (78.200%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.489841\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.516415\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.483989\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.498342\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.495678\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.505265\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.494171\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.497790\n",
      "\n",
      "Test set: Average loss: 0.4675, Accuracy: 15635/20000 (78.175%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.501087\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.504161\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.490667\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.493460\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.475611\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.496312\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.520442\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.484595\n",
      "\n",
      "Test set: Average loss: 0.4644, Accuracy: 15683/20000 (78.415%)\n",
      "\n",
      "\n",
      " training DNN with 100000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.710034\n",
      "Train Epoch: 1 [10000/80000 (12%)]\tLoss: 0.636905\n",
      "Train Epoch: 1 [20000/80000 (25%)]\tLoss: 0.593469\n",
      "Train Epoch: 1 [30000/80000 (38%)]\tLoss: 0.542106\n",
      "Train Epoch: 1 [40000/80000 (50%)]\tLoss: 0.534986\n",
      "Train Epoch: 1 [50000/80000 (62%)]\tLoss: 0.510867\n",
      "Train Epoch: 1 [60000/80000 (75%)]\tLoss: 0.530152\n",
      "Train Epoch: 1 [70000/80000 (88%)]\tLoss: 0.514187\n",
      "\n",
      "Test set: Average loss: 0.4690, Accuracy: 15615/20000 (78.075%)\n",
      "\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.466206\n",
      "Train Epoch: 2 [10000/80000 (12%)]\tLoss: 0.503139\n",
      "Train Epoch: 2 [20000/80000 (25%)]\tLoss: 0.507244\n",
      "Train Epoch: 2 [30000/80000 (38%)]\tLoss: 0.505396\n",
      "Train Epoch: 2 [40000/80000 (50%)]\tLoss: 0.500601\n",
      "Train Epoch: 2 [50000/80000 (62%)]\tLoss: 0.493707\n",
      "Train Epoch: 2 [60000/80000 (75%)]\tLoss: 0.473309\n",
      "Train Epoch: 2 [70000/80000 (88%)]\tLoss: 0.465034\n",
      "\n",
      "Test set: Average loss: 0.4562, Accuracy: 15767/20000 (78.835%)\n",
      "\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.483578\n",
      "Train Epoch: 3 [10000/80000 (12%)]\tLoss: 0.488034\n",
      "Train Epoch: 3 [20000/80000 (25%)]\tLoss: 0.468584\n",
      "Train Epoch: 3 [30000/80000 (38%)]\tLoss: 0.464624\n",
      "Train Epoch: 3 [40000/80000 (50%)]\tLoss: 0.475215\n",
      "Train Epoch: 3 [50000/80000 (62%)]\tLoss: 0.453138\n",
      "Train Epoch: 3 [60000/80000 (75%)]\tLoss: 0.469072\n",
      "Train Epoch: 3 [70000/80000 (88%)]\tLoss: 0.466502\n",
      "\n",
      "Test set: Average loss: 0.4526, Accuracy: 15798/20000 (78.990%)\n",
      "\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.447405\n",
      "Train Epoch: 4 [10000/80000 (12%)]\tLoss: 0.455803\n",
      "Train Epoch: 4 [20000/80000 (25%)]\tLoss: 0.473606\n",
      "Train Epoch: 4 [30000/80000 (38%)]\tLoss: 0.448837\n",
      "Train Epoch: 4 [40000/80000 (50%)]\tLoss: 0.470335\n",
      "Train Epoch: 4 [50000/80000 (62%)]\tLoss: 0.488907\n",
      "Train Epoch: 4 [60000/80000 (75%)]\tLoss: 0.440853\n",
      "Train Epoch: 4 [70000/80000 (88%)]\tLoss: 0.475955\n",
      "\n",
      "Test set: Average loss: 0.4495, Accuracy: 15863/20000 (79.315%)\n",
      "\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.492592\n",
      "Train Epoch: 5 [10000/80000 (12%)]\tLoss: 0.444875\n",
      "Train Epoch: 5 [20000/80000 (25%)]\tLoss: 0.508808\n",
      "Train Epoch: 5 [30000/80000 (38%)]\tLoss: 0.454961\n",
      "Train Epoch: 5 [40000/80000 (50%)]\tLoss: 0.426661\n",
      "Train Epoch: 5 [50000/80000 (62%)]\tLoss: 0.436331\n",
      "Train Epoch: 5 [60000/80000 (75%)]\tLoss: 0.481597\n",
      "Train Epoch: 5 [70000/80000 (88%)]\tLoss: 0.429632\n",
      "\n",
      "Test set: Average loss: 0.4475, Accuracy: 15880/20000 (79.400%)\n",
      "\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.477526\n",
      "Train Epoch: 6 [10000/80000 (12%)]\tLoss: 0.449983\n",
      "Train Epoch: 6 [20000/80000 (25%)]\tLoss: 0.437637\n",
      "Train Epoch: 6 [30000/80000 (38%)]\tLoss: 0.436613\n",
      "Train Epoch: 6 [40000/80000 (50%)]\tLoss: 0.459004\n",
      "Train Epoch: 6 [50000/80000 (62%)]\tLoss: 0.480151\n",
      "Train Epoch: 6 [60000/80000 (75%)]\tLoss: 0.445661\n",
      "Train Epoch: 6 [70000/80000 (88%)]\tLoss: 0.463288\n",
      "\n",
      "Test set: Average loss: 0.4472, Accuracy: 15904/20000 (79.520%)\n",
      "\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.449816\n",
      "Train Epoch: 7 [10000/80000 (12%)]\tLoss: 0.437155\n",
      "Train Epoch: 7 [20000/80000 (25%)]\tLoss: 0.476608\n",
      "Train Epoch: 7 [30000/80000 (38%)]\tLoss: 0.456227\n",
      "Train Epoch: 7 [40000/80000 (50%)]\tLoss: 0.427921\n",
      "Train Epoch: 7 [50000/80000 (62%)]\tLoss: 0.424769\n",
      "Train Epoch: 7 [60000/80000 (75%)]\tLoss: 0.484700\n",
      "Train Epoch: 7 [70000/80000 (88%)]\tLoss: 0.461422\n",
      "\n",
      "Test set: Average loss: 0.4451, Accuracy: 15924/20000 (79.620%)\n",
      "\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.464651\n",
      "Train Epoch: 8 [10000/80000 (12%)]\tLoss: 0.438999\n",
      "Train Epoch: 8 [20000/80000 (25%)]\tLoss: 0.448219\n",
      "Train Epoch: 8 [30000/80000 (38%)]\tLoss: 0.421540\n",
      "Train Epoch: 8 [40000/80000 (50%)]\tLoss: 0.472870\n",
      "Train Epoch: 8 [50000/80000 (62%)]\tLoss: 0.457790\n",
      "Train Epoch: 8 [60000/80000 (75%)]\tLoss: 0.477810\n",
      "Train Epoch: 8 [70000/80000 (88%)]\tLoss: 0.438413\n",
      "\n",
      "Test set: Average loss: 0.4444, Accuracy: 15901/20000 (79.505%)\n",
      "\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.432571\n",
      "Train Epoch: 9 [10000/80000 (12%)]\tLoss: 0.455962\n",
      "Train Epoch: 9 [20000/80000 (25%)]\tLoss: 0.423890\n",
      "Train Epoch: 9 [30000/80000 (38%)]\tLoss: 0.480414\n",
      "Train Epoch: 9 [40000/80000 (50%)]\tLoss: 0.437187\n",
      "Train Epoch: 9 [50000/80000 (62%)]\tLoss: 0.467119\n",
      "Train Epoch: 9 [60000/80000 (75%)]\tLoss: 0.483969\n",
      "Train Epoch: 9 [70000/80000 (88%)]\tLoss: 0.457697\n",
      "\n",
      "Test set: Average loss: 0.4442, Accuracy: 15929/20000 (79.645%)\n",
      "\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.453582\n",
      "Train Epoch: 10 [10000/80000 (12%)]\tLoss: 0.449087\n",
      "Train Epoch: 10 [20000/80000 (25%)]\tLoss: 0.466515\n",
      "Train Epoch: 10 [30000/80000 (38%)]\tLoss: 0.438602\n",
      "Train Epoch: 10 [40000/80000 (50%)]\tLoss: 0.445478\n",
      "Train Epoch: 10 [50000/80000 (62%)]\tLoss: 0.452407\n",
      "Train Epoch: 10 [60000/80000 (75%)]\tLoss: 0.452849\n",
      "Train Epoch: 10 [70000/80000 (88%)]\tLoss: 0.461075\n",
      "\n",
      "Test set: Average loss: 0.4438, Accuracy: 15899/20000 (79.495%)\n",
      "\n",
      "Training on 160000 examples\n",
      "Using both high and low level features\n",
      "Testing on 40000 examples\n",
      "Using both high and low level features\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.000010. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.705378\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.705996\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.707956\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.703669\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.707292\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.705640\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.703749\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.706360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7001, Accuracy: 17602/40000 (44.005%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.705975\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.708230\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.706395\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.704845\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.702706\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.706650\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.705066\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.706176\n",
      "\n",
      "Test set: Average loss: 0.7000, Accuracy: 17630/40000 (44.075%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.705903\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.705434\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.705613\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.701805\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.701164\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.706852\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.707321\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.702315\n",
      "\n",
      "Test set: Average loss: 0.6999, Accuracy: 17657/40000 (44.142%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.699055\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.708534\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.706195\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.705924\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.705723\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.706754\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.705038\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.707561\n",
      "\n",
      "Test set: Average loss: 0.6998, Accuracy: 17699/40000 (44.248%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.703754\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.698444\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.702047\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.699995\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.712333\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.708527\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.704029\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.703487\n",
      "\n",
      "Test set: Average loss: 0.6997, Accuracy: 17728/40000 (44.320%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.708110\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.705622\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.702876\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.707062\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.706936\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.695657\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.704963\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.706326\n",
      "\n",
      "Test set: Average loss: 0.6996, Accuracy: 17756/40000 (44.390%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.705031\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.702605\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.706703\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.707784\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.702357\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.701669\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.702067\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.709155\n",
      "\n",
      "Test set: Average loss: 0.6995, Accuracy: 17802/40000 (44.505%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.705571\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.704526\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.705118\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.703757\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.709261\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.705016\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.706421\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.700066\n",
      "\n",
      "Test set: Average loss: 0.6993, Accuracy: 17840/40000 (44.600%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.705187\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.704544\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.703321\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.706861\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.700266\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.703421\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.703746\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.702682\n",
      "\n",
      "Test set: Average loss: 0.6992, Accuracy: 17869/40000 (44.672%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.703363\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.705391\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.702652\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.703468\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.698541\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.706441\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.704389\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.703209\n",
      "\n",
      "Test set: Average loss: 0.6991, Accuracy: 17915/40000 (44.788%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.000100. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.698396\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.698866\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.701402\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.698021\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.698192\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.695451\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.699390\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.701555\n",
      "\n",
      "Test set: Average loss: 0.6896, Accuracy: 20552/40000 (51.380%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.699959\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.697329\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.691820\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.698557\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.693670\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.694286\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.698711\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.697711\n",
      "\n",
      "Test set: Average loss: 0.6883, Accuracy: 20812/40000 (52.030%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.698032\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.697427\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.689850\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.695308\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.695276\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.697952\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.691854\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.694932\n",
      "\n",
      "Test set: Average loss: 0.6870, Accuracy: 21048/40000 (52.620%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.696920\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.689578\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.697653\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.689913\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.690594\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.697171\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.695638\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.695573\n",
      "\n",
      "Test set: Average loss: 0.6858, Accuracy: 21366/40000 (53.415%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.687915\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.691525\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.693380\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.692439\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.695893\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.692090\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.692299\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.692883\n",
      "\n",
      "Test set: Average loss: 0.6845, Accuracy: 21639/40000 (54.097%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.696120\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.695128\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.689633\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.692345\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.691020\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.693841\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.689989\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.689109\n",
      "\n",
      "Test set: Average loss: 0.6834, Accuracy: 21910/40000 (54.775%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.691662\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.690945\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.693215\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.688376\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.688045\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.688315\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.687197\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.689261\n",
      "\n",
      "Test set: Average loss: 0.6822, Accuracy: 22192/40000 (55.480%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.687703\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.689070\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.685811\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.688492\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.688515\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.689583\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.686230\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.686407\n",
      "\n",
      "Test set: Average loss: 0.6810, Accuracy: 22530/40000 (56.325%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.687456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.686028\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.689264\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.688446\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.686449\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.691495\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.687447\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.691844\n",
      "\n",
      "Test set: Average loss: 0.6799, Accuracy: 22882/40000 (57.205%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.689990\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.687099\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.681460\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.687270\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.685684\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.683231\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.687093\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.682962\n",
      "\n",
      "Test set: Average loss: 0.6788, Accuracy: 23206/40000 (58.015%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.001000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.689108\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.697128\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.689239\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.685374\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.686474\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.690089\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.680788\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.680080\n",
      "\n",
      "Test set: Average loss: 0.6669, Accuracy: 25865/40000 (64.662%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.677803\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.683846\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.676049\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.683029\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.670379\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.675241\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.668600\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.670031\n",
      "\n",
      "Test set: Average loss: 0.6556, Accuracy: 27886/40000 (69.715%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.667766\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.665894\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.669676\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.664186\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.663084\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.670774\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.661623\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.666345\n",
      "\n",
      "Test set: Average loss: 0.6457, Accuracy: 28856/40000 (72.140%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.662481\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.657399\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.656659\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.656055\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.663333\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.648718\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.654862\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.653988\n",
      "\n",
      "Test set: Average loss: 0.6363, Accuracy: 29291/40000 (73.228%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.647755\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.653437\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.646983\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.648828\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.645205\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.647996\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.652728\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.640562\n",
      "\n",
      "Test set: Average loss: 0.6276, Accuracy: 29561/40000 (73.903%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.641107\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.645551\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.641293\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.640683\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.638214\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.638726\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.646894\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.642301\n",
      "\n",
      "Test set: Average loss: 0.6193, Accuracy: 29771/40000 (74.427%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.639891\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.645752\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.632307\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.635306\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.638025\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.632357\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.632027\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.636974\n",
      "\n",
      "Test set: Average loss: 0.6114, Accuracy: 29905/40000 (74.763%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.634868\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.629932\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.628609\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.625712\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.626740\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.634062\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.629881\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.625859\n",
      "\n",
      "Test set: Average loss: 0.6038, Accuracy: 29995/40000 (74.987%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.625145\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.626572\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.620243\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.619444\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.627936\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.627460\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.627686\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.613311\n",
      "\n",
      "Test set: Average loss: 0.5963, Accuracy: 30060/40000 (75.150%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.615407\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.623704\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.619627\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.620479\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.608451\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.617685\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.612309\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.616382\n",
      "\n",
      "Test set: Average loss: 0.5892, Accuracy: 30130/40000 (75.325%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.010000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.698379\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.679422\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.674136\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.668516\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.658109\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.650219\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.638263\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.636533\n",
      "\n",
      "Test set: Average loss: 0.6055, Accuracy: 30074/40000 (75.185%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.624888\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.616865\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.616839\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.605123\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.611440\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.590654\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.585734\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.598598\n",
      "\n",
      "Test set: Average loss: 0.5457, Accuracy: 30328/40000 (75.820%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.568540\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.585310\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.582293\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.582939\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.578519\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.571420\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.564026\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.564366\n",
      "\n",
      "Test set: Average loss: 0.5134, Accuracy: 30649/40000 (76.623%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.557406\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.552144\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.571896\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.546366\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.545914\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.540634\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.548739\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.557149\n",
      "\n",
      "Test set: Average loss: 0.4952, Accuracy: 30867/40000 (77.168%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.547409\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.542211\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.556387\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.543752\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.550467\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.530864\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.539643\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.532589\n",
      "\n",
      "Test set: Average loss: 0.4841, Accuracy: 31051/40000 (77.627%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.540473\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.521824\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.533043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.512980\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.528996\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.524936\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.528913\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.530016\n",
      "\n",
      "Test set: Average loss: 0.4755, Accuracy: 31220/40000 (78.050%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.524247\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.520490\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.511003\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.505872\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.516610\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.517434\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.518095\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.510188\n",
      "\n",
      "Test set: Average loss: 0.4706, Accuracy: 31275/40000 (78.188%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.500267\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.515883\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.509539\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.495808\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.495005\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.478631\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.503356\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.502617\n",
      "\n",
      "Test set: Average loss: 0.4655, Accuracy: 31367/40000 (78.418%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.509758\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.504839\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.518935\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.492440\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.514420\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.487443\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.515199\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.498435\n",
      "\n",
      "Test set: Average loss: 0.4619, Accuracy: 31455/40000 (78.638%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.517930\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.481739\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.502270\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.493346\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.485037\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.482217\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.496745\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.501032\n",
      "\n",
      "Test set: Average loss: 0.4591, Accuracy: 31504/40000 (78.760%)\n",
      "\n",
      "\n",
      " training DNN with 200000 data points and SGD lr=0.100000. \n",
      "\n",
      "Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.720915\n",
      "Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.643371\n",
      "Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.591666\n",
      "Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.571699\n",
      "Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.530918\n",
      "Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.534381\n",
      "Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.518941\n",
      "Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.490676\n",
      "\n",
      "Test set: Average loss: 0.4646, Accuracy: 31375/40000 (78.438%)\n",
      "\n",
      "Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.510355\n",
      "Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.494206\n",
      "Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.478734\n",
      "Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.491785\n",
      "Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.468593\n",
      "Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.464382\n",
      "Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.479111\n",
      "Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.486017\n",
      "\n",
      "Test set: Average loss: 0.4502, Accuracy: 31691/40000 (79.228%)\n",
      "\n",
      "Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.466499\n",
      "Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.461259\n",
      "Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.470140\n",
      "Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.462802\n",
      "Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.470961\n",
      "Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.470122\n",
      "Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.467336\n",
      "Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.467999\n",
      "\n",
      "Test set: Average loss: 0.4436, Accuracy: 31836/40000 (79.590%)\n",
      "\n",
      "Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.457593\n",
      "Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.474765\n",
      "Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.456292\n",
      "Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.480981\n",
      "Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.463837\n",
      "Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.458277\n",
      "Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.466906\n",
      "Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.455782\n",
      "\n",
      "Test set: Average loss: 0.4405, Accuracy: 31860/40000 (79.650%)\n",
      "\n",
      "Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.470410\n",
      "Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.480170\n",
      "Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.468328\n",
      "Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.438006\n",
      "Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.453472\n",
      "Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.468086\n",
      "Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.458702\n",
      "Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.447293\n",
      "\n",
      "Test set: Average loss: 0.4379, Accuracy: 31918/40000 (79.795%)\n",
      "\n",
      "Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.465341\n",
      "Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.453369\n",
      "Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.465097\n",
      "Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.449867\n",
      "Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.448615\n",
      "Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.460972\n",
      "Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.449854\n",
      "Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.459059\n",
      "\n",
      "Test set: Average loss: 0.4369, Accuracy: 31958/40000 (79.895%)\n",
      "\n",
      "Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.444441\n",
      "Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.460648\n",
      "Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.472311\n",
      "Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.445322\n",
      "Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.458156\n",
      "Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.445797\n",
      "Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.435844\n",
      "Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.456504\n",
      "\n",
      "Test set: Average loss: 0.4375, Accuracy: 31921/40000 (79.802%)\n",
      "\n",
      "Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.465196\n",
      "Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.457353\n",
      "Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.450719\n",
      "Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.447780\n",
      "Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.451770\n",
      "Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.443656\n",
      "Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.470793\n",
      "Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.460126\n",
      "\n",
      "Test set: Average loss: 0.4349, Accuracy: 31961/40000 (79.903%)\n",
      "\n",
      "Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.457658\n",
      "Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.451972\n",
      "Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.454635\n",
      "Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.457690\n",
      "Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.448219\n",
      "Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.456366\n",
      "Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.446367\n",
      "Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.451008\n",
      "\n",
      "Test set: Average loss: 0.4338, Accuracy: 31968/40000 (79.920%)\n",
      "\n",
      "Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.432577\n",
      "Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.455031\n",
      "Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.473294\n",
      "Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.462015\n",
      "Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.457204\n",
      "Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.444788\n",
      "Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.429693\n",
      "Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.440606\n",
      "\n",
      "Test set: Average loss: 0.4342, Accuracy: 31961/40000 (79.903%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\telux\\AppData\\Local\\Temp\\ipykernel_20748\\2156524258.py:322: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(['']+x)\n",
      "C:\\Users\\telux\\AppData\\Local\\Temp\\ipykernel_20748\\2156524258.py:323: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['']+y)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "from torchvision import datasets # load data\n",
    "\n",
    "class SUSY_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"SUSY pytorch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_file, root_dir, dataset_size, train=True, transform=None, high_level_feats=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            train (bool, optional): If set to `True` load training data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            high_level_festures (bool, optional): If set to `True`, working with high-level features only. \n",
    "                                        If set to `False`, working with low-level features only.\n",
    "                                        Default is `None`: working with all features\n",
    "        \"\"\"\n",
    "\n",
    "        import pandas as pd\n",
    " \n",
    "        features=['SUSY','lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    "                'missing energy magnitude', 'missing energy phi', 'MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2', \n",
    "                'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "        low_features=['lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    "                'missing energy magnitude', 'missing energy phi']\n",
    "\n",
    "        high_features=['MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2','S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "\n",
    "        #Number of datapoints to work with\n",
    "        df = pd.read_csv(root_dir+data_file, header=None,nrows=dataset_size,engine='python')\n",
    "        df.columns=features\n",
    "        Y = df['SUSY']\n",
    "        X = df[[col for col in df.columns if col!=\"SUSY\"]]\n",
    "\n",
    "        # set training and test data size\n",
    "        train_size=int(0.8*dataset_size)\n",
    "        self.train=train\n",
    "\n",
    "        if self.train:\n",
    "            X=X[:train_size]\n",
    "            Y=Y[:train_size]\n",
    "            print(\"Training on {} examples\".format(train_size))\n",
    "        else:\n",
    "            X=X[train_size:]\n",
    "            Y=Y[train_size:]\n",
    "            print(\"Testing on {} examples\".format(dataset_size-train_size))\n",
    "\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # make datasets using only the 8 low-level features and 10 high-level features\n",
    "        if high_level_feats is None:\n",
    "            self.data=(X.values.astype(np.float32),Y.values.astype(int))\n",
    "            print(\"Using both high and low level features\")\n",
    "        elif high_level_feats is True:\n",
    "            self.data=(X[high_features].values.astype(np.float32),Y.values.astype(int))\n",
    "            print(\"Using both high-level features only.\")\n",
    "        elif high_level_feats is False:\n",
    "            self.data=(X[low_features].values.astype(np.float32),Y.values.astype(int))\n",
    "            print(\"Using both low-level features only.\")\n",
    "\n",
    "\n",
    "    # override __len__ and __getitem__ of the Dataset() class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample=(self.data[0][idx,...],self.data[1][idx])\n",
    "\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "def load_data(args):\n",
    "\n",
    "    data_file='SUSY.csv'\n",
    "    root_dir=os.path.expanduser('~')+'/Homework Computing physics/Data/'\n",
    "\n",
    "    kwargs = {} # CUDA arguments, if enabled\n",
    "    # load and noralise train and test data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=True,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        SUSY_Dataset(data_file,root_dir,args.dataset_size,train=False,high_level_feats=args.high_level_feats),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "import torch.nn as nn # construct NN\n",
    "class model(nn.Module):\n",
    "    def __init__(self,high_level_feats=None):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        if high_level_feats is None:\n",
    "            self.fc1 = nn.Linear(18, 200) # all features\n",
    "        elif high_level_feats:\n",
    "            self.fc1 = nn.Linear(10, 200) # low-level only\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(8, 200) # high-level only\n",
    "\n",
    "\n",
    "        self.batchnorm1=nn.BatchNorm1d(200, eps=1e-05, momentum=0.1)\n",
    "        self.batchnorm2=nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(200, 100) # see forward function for dimensions\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Defines the feed-forward function for the NN.\n",
    "\n",
    "        A backward function is automatically defined using `torch.autograd`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : autograd.Tensor\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        autograd.Tensor\n",
    "            output layer of NN\n",
    "\n",
    "        '''\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # apply dropout\n",
    "        #x=self.batchnorm1(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply rectified linear unit\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # apply dropout\n",
    "        #x=self.batchnorm2(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "\n",
    "        # apply affine operation fc2\n",
    "        x = self.fc3(x)\n",
    "        # soft-max layer\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "\n",
    "def evaluate_model(args,train_loader,test_loader):\n",
    "\n",
    "    # create model\n",
    "    DNN = model(high_level_feats=args.high_level_feats)\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define SGD optimizer\n",
    "    optimizer = optim.SGD(DNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    #optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def train(epoch):\n",
    "        '''Trains a NN using minibatches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Training epoch number.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # set model to training mode (affects Dropout and BatchNorm)\n",
    "        DNN.train()\n",
    "        # loop over training data\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            # zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            # compute output of final layer: forward step\n",
    "            output = DNN(data)\n",
    "            # compute loss\n",
    "            loss = criterion(output, label.long())\n",
    "            # run backprop: backward step\n",
    "            loss.backward()\n",
    "            # update weigths of NN\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print loss at current epoch\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item() ))\n",
    "            \n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    ################################################\n",
    "\n",
    "    def test():\n",
    "        '''Tests NN performance.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # evaluate model\n",
    "        DNN.eval()\n",
    "\n",
    "        test_loss = 0 # loss function on test data\n",
    "        correct = 0 # number of correct predictions\n",
    "        # loop over test data\n",
    "        for data, label in test_loader:\n",
    "            # compute model prediction softmax probability\n",
    "            output = DNN(data)\n",
    "            # compute test loss\n",
    "            test_loss += criterion(output, label.long(), size_average=False).item() # sum up batch loss\n",
    "            # find most likely prediction\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            # update number of correct predictions\n",
    "            correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        # print test loss\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "\n",
    "        return test_loss, correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    train_loss=np.zeros((args.epochs,))\n",
    "    test_loss=np.zeros_like(train_loss)\n",
    "    test_accuracy=np.zeros_like(train_loss)\n",
    "\n",
    "    epochs=range(1, args.epochs + 1)\n",
    "    for epoch in epochs:\n",
    "\n",
    "        train_loss[epoch-1] = train(epoch)\n",
    "        test_loss[epoch-1], test_accuracy[epoch-1] = test()\n",
    "\n",
    "\n",
    "\n",
    "    return test_loss[-1], test_accuracy[-1]\n",
    "\n",
    "def grid_search(args):\n",
    "\n",
    "\n",
    "    # perform grid search over learnign rate and number of hidden neurons\n",
    "    dataset_sizes=[1000, 10000, 100000, 200000] #np.logspace(2,5,4).astype('int')\n",
    "    learning_rates=np.logspace(-5,-1,5)\n",
    "\n",
    "    # pre-alocate data\n",
    "    test_loss=np.zeros((len(dataset_sizes),len(learning_rates)),dtype=np.float64)\n",
    "    test_accuracy=np.zeros_like(test_loss)\n",
    "\n",
    "    # do grid search\n",
    "    for i, dataset_size in enumerate(dataset_sizes):\n",
    "        # upate data set size parameters\n",
    "        args.dataset_size=dataset_size\n",
    "        args.batch_size=int(0.01*dataset_size)\n",
    "\n",
    "        # load data\n",
    "        train_loader, test_loader = load_data(args)\n",
    "\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            # update learning rate\n",
    "            args.lr=lr\n",
    "\n",
    "            print(\"\\n training DNN with %5d data points and SGD lr=%0.6f. \\n\" %(dataset_size,lr) )\n",
    "\n",
    "            test_loss[i,j],test_accuracy[i,j] = evaluate_model(args,train_loader,test_loader)\n",
    "\n",
    "\n",
    "    plot_data(learning_rates,dataset_sizes,test_accuracy)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(x,y,data):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('accuracy (%)',rotation=90,fontsize=fontsize)\n",
    "    cbar.set_ticks([0,.2,.4,0.6,0.8,1.0])\n",
    "    cbar.set_ticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch SUSY Example')\n",
    "parser.add_argument('--dataset_size', type=int, default=100000, metavar='DS',\n",
    "                help='size of data set (default: 100000)')\n",
    "parser.add_argument('--high_level_feats', type=bool, default=None, metavar='HLF',\n",
    "                help='toggles high level features (default: None)')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.05, metavar='LR',\n",
    "                help='learning rate (default: 0.02)')\n",
    "parser.add_argument('--momentum', type=float, default=0.8, metavar='M',\n",
    "                help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=2, metavar='S',\n",
    "                help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# set seed of random number generator\n",
    "torch.manual_seed(args.seed)\n",
    "# args:dataset_size=100000, high_level_feats=None, batch_size=100, test_batch_size=1000, epochs=10, \n",
    "#      lr=0.05, momentum=0.8, no_cuda=False, seed=2, log_interval=10\n",
    "\n",
    "grid_search(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412325a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
